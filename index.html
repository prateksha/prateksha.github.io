<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Prateksha Udhayanan</title>

    <meta name="author" content="Prateksha Udhayanan">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Prateksha Udhayanan
                </p>
                
                <p>
                    I am a second-year PhD student in the Department of Computer Science at the <a href="https://www.cs.umd.edu/">University of Maryland</a>, advised by <a href="https://www.cs.umd.edu/~abhinav/" target="_blank">Prof. Abhinav Shrivastava</a>. 
                    Before joining UMD, I worked as a Research Associate at <a href="https://research.adobe.com/" target="_blank">Adobe Research</a>, where I worked on projects spanning retrieval, editing, and generation for images, videos, and graphic designs. 
                </p>
                <p>
                  I received my Bachelor’s and Master’s in Computer Science Engineering with a specialization in Artificial Intelligence and Machine Learning from <a href="https://www.iiitb.ac.in/" target="_blank">IIIT Bangalore</a>. 
                  During my undergraduate studies, I spent three months as a research intern at <a href="https://research.adobe.com/" target="_blank"> Adobe Research</a>, working 
                  with <a href="https://research.adobe.com/person/balaji-vasan-srinivasan/" target="_blank"> Balaji Vasan Srinivasan</a> and <a href="https://research.adobe.com/person/stefano-petrangeli/" target="_blank"> Stefano Petrangeli</a> on 
                  document-to-video transformation. I also worked on anomaly detection in procedural videos, with <a href="https://www.linkedin.com/in/varghese-alex-74448567/">Varghese Alex</a> and <a href="https://www.linkedin.com/in/vinay-sudhakaran/">Vinay Sudhakaran</a> during my summer internship at <a href="https://www.siemens.com/global/en.html">Siemens</a>. 
                  I had the opportunity to explore various projects during my undergraduate studies with <a href="https://www.iiitb.ac.in/faculty/dinesh-babu-jayagopi">Prof. Dinesh Babu Jayagopi</a>, including analyzing speaker behaviour in videos for virtual meeting scenarios and 
                  hand-gesture estimation for Indian Sign Language synthesis on human avatars.
                </p>

                <p style="text-align:center">
                  <!-- <a href="mailto:pratekshau@gmail.com">Email</a> &nbsp;/&nbsp; -->
                  <a href="data/CV_Prateksha.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=IMX3NNYAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/prateksha-udhayanan-561b00155/">LinkedIn</a> 
                  <!-- <a href="https://github.com/prateksha/">Github</a> -->
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/Prateksha_2_cropped.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/Prateksha_2_cropped.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interests are in computer vision, with a primary focus on generative models. I am currently working on image generation models, 
                  and have previously worked on adding controllability to the image generation process and evaluating motion quality in video generation. 
                  <!-- In general, I am always excited to work on challenging problems centered around pixels! -->
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <!-- Paper: Design-o-meter: Towards Evaluating and Refining Graphic Designs -->
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/design_o_meter.png" alt="clean-usnob" width="160" height="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/WACV2025/papers/Goyal_Design-o-Meter_Towards_Evaluating_and_Refining_Graphic_Designs_WACV_2025_paper.pdf">
                  <span class="papertitle">Design-o-meter: Towards Evaluating and Refining Graphic Designs</span>
                </a>
                <br>
                  <a href="https://sahilg06.github.io/">Sahil Goyal</a>, Abhinav Mahajan, <a href="https://swastishreya.github.io/">Swasti S. Mishra</a>, <strong>Prateksha Udhayanan</strong>, 
                  Tripti Shukla, <a href="https://josephkj.in/">KJ Joseph</a>, 
                  <a href="https://research.adobe.com/person/balaji-vasan-srinivasan/">Balaji Vasan Srinivasan</a>
                <br>
                <em>WACV 2025</em>
                <p>We propose Design-o-meter, a data-driven methodology to score and refine graphic designs.</p>
              </td>
            </tr>
            <!-- Paper: CoPL: Contextual Prompt Learning for Vision-Language Understanding -->
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/copl_square.png" alt="clean-usnob" width="160" height="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29766">
                  <span class="papertitle">CoPL: Contextual Prompt Learning for Vision-Language Understanding</span>
                </a>
                <br>
                  <a href="https://research.adobe.com/person/koustava-goswami/">Koustava Goswami</a>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <strong>Prateksha Udhayanan</strong>, <a href="https://josephkj.in/">KJ Joseph</a>, 
                  <a href="https://research.adobe.com/person/balaji-vasan-srinivasan/">Balaji Vasan Srinivasan</a>
                <br>
                <em>AAAI 2024</em>
                <p>We propose Contextualized Prompt Learning (CoPL), a prompt learning method that adapts prompt weights dynamically and aligns the prompt vectors with local image features.</p>
              </td>
            </tr>
            <!-- Paper: Iterative multi-granular image editing using diffusion models -->
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/emilie_figure_square.png" alt="clean-usnob" width="160" height="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/WACV2024/papers/Joseph_Iterative_Multi-Granular_Image_Editing_Using_Diffusion_Models_WACV_2024_paper.pdf">
                  <span class="papertitle">Iterative multi-granular image editing using diffusion models</span>
                </a>
                <br>
                  <a href="https://josephkj.in/">KJ Joseph</a>, <strong>Prateksha Udhayanan</strong>, Tripti Shukla, 
                  <a href="https://aisagarw.github.io/">Aishwarya Agarwal</a>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="https://research.adobe.com/person/koustava-goswami/">Koustava Goswami</a>, <a href="https://research.adobe.com/person/balaji-vasan-srinivasan/">Balaji Vasan Srinivasan</a>
                <br>
                <em>WACV 2024</em>
                <p>We present a training-free framework for iterative, multi-granular image editing, along with IMIE-Bench, a new benchmark dataset for evaluating the proposed task.</p>
              </td>
            </tr>
            <!-- Paper: Recipe2Video: Synthesizing Personalized Videos from Recipe Texts -->
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/recipe2video_square.png" alt="clean-usnob" width="160" height="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Udhayanan_Recipe2Video_Synthesizing_Personalized_Videos_From_Recipe_Texts_WACV_2023_paper.pdf">
                  <span class="papertitle">Recipe2Video: Synthesizing Personalized Videos from Recipe Texts</span>
                </a>
                <br>
                  <strong>Prateksha Udhayanan</strong>, <a href="https://www.linkedin.com/in/suryateja-bulusu/?originalSubdomain=in">Suryateja BV</a>, 
                  Parth Laturia, Dev Chauhan, Darshan Khandelwal, <a href="https://research.adobe.com/person/stefano-petrangeli/">Stefano Petrangeli</a>, <a href="https://research.adobe.com/person/balaji-vasan-srinivasan/">Balaji Vasan Srinivasan</a>
                <br>
                <em>WACV 2023</em>
                <p> We present a novel deep-learning driven system - Recipe2Video that automatically converts a recipe document into a multimodal illustrative video.</p>
              </td>
            </tr>
            <!-- arXiv: Learning with multi-modal gradient attention for explainable composed image retrieval -->
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/mmgrad_cir_square.png" alt="clean-usnob" width="160" height="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2308.16649">
                  <span class="papertitle">Learning with multi-modal gradient attention for explainable composed image retrieval</span>
                </a>
                <br>
                  <strong>Prateksha Udhayanan</strong>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, 
                  <a href="https://research.adobe.com/person/balaji-vasan-srinivasan/">Balaji Vasan Srinivasan</a>
                <br>
                <em>arXiv</em>
                <p> We propose a gradient-attention-based learning objective for composed image retrieval, that explicitly forces the model to focus on the local regions of interest being modified in each retrieval step.</p>
              </td>
            </tr>
            <!-- Multimodal Unsupervised Domain Adaptation for Predicting Speaker Characteristics from Video -->
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/mmuda_figure_square.png" alt="clean-usnob" width="160" height="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://link.springer.com/article/10.1007/s42979-024-02723-6">
                  <span class="papertitle">Multimodal Unsupervised Domain Adaptation for Predicting Speaker Characteristics from Video</span>
                </a>
                <br>
                  Chinchu Thomas, <strong>Prateksha Udhayanan</strong> , Ayush Yadav, Seethamraju Purvaj, Dinesh Babu Jayagopi
                <br>
                <em>SN Computer Science 2024</em>
                <p>We propose a multimodal unsupervised domain adaptation method to predict the persuasiveness and expertise of the speaker in a video</p>
              </td>
            </tr>
            <!-- Source-code similarity measurement: syntax tree fingerprinting for automated evaluation -->
             <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/ast_ssm_square.png" alt="clean-usnob" width="160" height="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://dl.acm.org/doi/pdf/10.1145/3486001.3486228">
                  <span class="papertitle">Source-Code Similarity Measurement: Syntax Tree Fingerprinting for Automated Evaluation</span>
                </a>
                <br>
                  Arjun Verma, <strong>Prateksha Udhayanan</strong>, Rahul Murali Shankar, Nikhila Kn, Sujit Kumar Chakrabarti
                <br>
                <em>AI-ML Systems 2021</em>
                <p> We propose an AST-based method to compute similarity score between two source codes, focusing on their structure rather than their functional outputs.</p>
              </td>
            </tr>
          </tbody></table>

          
					<!-- <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #fcb97d;">
								 <h2>Micropapers</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr>

            


            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #aaba9e;">
								 <h2>Recorded Talks</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://www.youtube.com/watch?v=hFlF33JZbA0">Radiance Fields and the Future of Generative Media, 2025</a><br>				  
                <a href="https://www.youtube.com/watch?v=h9vq_65eDas">View Dependent Podcast, 2024</a><br>
                <a href="https://www.youtube.com/watch?v=4tDhYsFuEqo">Bay Area Robotics Symposium, 2023
</a><br>
                <a href="https://youtu.be/TvWkwDYtBP4?t=7604">EGSR Keynote, 2021</a><br>
				<a href="https://www.youtube.com/watch?v=nRyOzHpcr4Q">TUM AI Lecture Series, 2020</a><br>
				<a href="https://www.youtube.com/watch?v=HfJpQCBTqZs">Vision & Graphics Seminar at MIT, 2020</a>
              </td>
            </tr>

            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h2>Academic Service</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://iccv.thecvf.com/">Lead Area Chair, ICCV 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2025/Organizers">Lead Area Chair, CVPR 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>
                <br>
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr>
						
						
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #edd892;">
								 <h2>Teaching</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
                <br>
                <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
              </td>
            </tr> -->
            

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Source Code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a> 
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
